import os
from typing import List, Union, Optional

import numpy as np
import torch

import equistore
from equistore import Labels, TensorBlock, TensorMap
from equistore.operations import _utils

# TODO:
# - Remove functions once in equistore:
#       - searchable_labels
#       - drop_blocks
#       - labels_equal
#       - equal_metadata

# ===== tensors to torch fxns


def tensors_to_torch(
    tensors: List[TensorMap],
    requires_grad: bool,
    dtype: torch.dtype,
    device: torch.device,
) -> list:
    """
    Takes a list of TensorMap objects and returns a new list of TensorMap
    objects where the values arrays of each block have been converted to
    torch.tensor objects of specified dtype (recommended and default
    torch.float64). If ``requires_grad=True``, torch's autograd algorithm will
    record operations on the values tensors when they are executed.

    :param tensors: a ``list`` of ``TensorMaps`` whose block values should be
        converted to torch.
    :param requires_grad: bool, whether or not to torch's autograd should record
        operations on this tensor.
    :param dtype: ``torch.dtype``, the base dtype of the resulting torch tensor,
        i.e. ``torch.float64``.
    :param device: ``torch.device``, the device on which the resulting torch
        tensor should be stored, i.e. ``torch.device("cpu")``.
    """

    return [
        tensor_to_torch(
            tensor,
            requires_grad=requires_grad,
            dtype=dtype,
            device=device,
        )
        for tensor in tensors
    ]


def tensor_to_torch(
    tensor: TensorMap,
    requires_grad: bool,
    dtype: torch.dtype,
    device: torch.device,
) -> TensorMap:
    """
    Creates a new :py:class:`TensorMap` where block values are
    PyTorch :py:class:`torch.tensor` objects. Assumes the block
    values are already as a type that is convertible to a
    :py:class:`torch.tensor`, such as a numpy array or RustNDArray.
    The resulting torch tensor dtypes are enforced as
    :py:class:`torch.float64`.

    :param tensor: input :py:class:`TensorMap`, with block values
        as ndarrays.
    :param requires_grad: bool, whether or not to torch's autograd should record
        operations on this tensor.
    :param dtype: ``torch.dtype``, the base dtype of the resulting torch tensor,
        i.e. ``torch.float64``.
    :param device: ``torch.device``, the device on which the resulting torch
        tensor should be stored, i.e. ``torch.device("cpu")``.

    :return: a :py:class:`TensorMap` where the values tensors of each
        block are now of type :py:class:`torch.tensor`.
    """

    return TensorMap(
        keys=tensor.keys,
        blocks=[
            block_to_torch(
                block,
                requires_grad=requires_grad,
                dtype=dtype,
                device=device,
            )
            for _, block in tensor
        ],
    )


def block_to_torch(
    block: TensorBlock,
    requires_grad: bool,
    dtype: torch.dtype,
    device: torch.device,
) -> TensorBlock:
    """
    Creates a new :py:class:`TensorBlock` where block values are PyTorch
    :py:class:`torch.tensor` objects. Assumes the block values are already as a
    type that is convertible to a :py:class:`torch.tensor`, such as a numpy
    array or RustNDArray. The resulting torch tensor dtypes are enforced as
    :py:class:`torch.float64`.

    :param block: input :py:class:`TensorBlock`, with block values as ndarrays.
    :param requires_grad: bool, whether or not to torch's autograd should record
        operations on this tensor.
    :param dtype: ``torch.dtype``, the base dtype of the resulting torch tensor,
        i.e. ``torch.float64``.
    :param device: ``torch.device``, the device on which the resulting torch
        tensor should be stored, i.e. ``torch.device("cpu")``.

    :return: a :py:class:`TensorBlock` whose values tensor is now of type
        :py:class:`torch.tensor`.
    """
    if isinstance(block.values, torch.Tensor):
        return block.copy()

    # Create new block, with the values tensor converted to a torch tensor.
    new_block = TensorBlock(
        values=torch.tensor(block.values, requires_grad=requires_grad, dtype=dtype).to(
            device.type
        ),
        samples=block.samples,
        components=block.components,
        properties=block.properties,
    )

    # Add gradients to each block, again where the values tensor are torch
    # tensors.
    for parameter, gradient in block.gradients():
        new_block.add_gradient(
            parameter,
            torch.tensor(gradient.data, requires_grad=requires_grad, dtype=dtype).to(
                device.type
            ),
            gradient.samples,
            gradient.components,
        )

    return new_block


def random_tensor_like(tensor: Union[TensorMap, TensorBlock, torch.Tensor]):
    """
    Returns a torch-based TensorMap, TensorBlock, or torch Tensor with the same
    shape and type as the input ``tensor``, but with random values.
    """
    # Input is a TensorMap, return a TensorMap
    if isinstance(tensor, TensorMap):
        new_blocks = []
        for key, block in tensor:

            new_block = TensorBlock(
                samples=block.samples,
                components=[c for c in block.components],
                properties=block.properties,
                values=torch.rand(block.values.shape),
            )
            new_blocks.append(new_block)
        return TensorMap(
            keys=tensor.keys,
            blocks=new_blocks,
        )
    # Input is a TensorBlock, return a TensorBlock
    elif isinstance(tensor, TensorBlock):
        return TensorBlock(
            samples=tensor.samples,
            components=[c for c in tensor.components],
            properties=tensor.properties,
            values=torch.rand(tensor.values.shape),
        )
    # Input is a torch Tensor, return a torch Tensor
    elif isinstance(tensor, torch.Tensor):
        return torch.rand(tensor.shape)
    else:
        raise TypeError("must pass either a TensorMap, TensorBlock, or torch Tensor")


# ===== tensors to numpy fxns


def tensor_to_numpy(tensor: TensorMap) -> TensorMap:
    """
    Takes a TensorMap object whose block values are torch.tensor objects and
    converts them to numpy arrays of dtype np.float64. Returns a new TensorMap
    object.
    """
    return TensorMap(
        keys=tensor.keys,
        blocks=[block_to_numpy(block) for _, block in tensor],
    )


def block_to_numpy(block: TensorBlock) -> TensorBlock:
    """
    Takes a TensorBlock object whose values are torch.tensor objects and
    converts them to numpy arrays of dtype np.float64. Returns a new TensorBlock
    object.
    """
    if isinstance(block.values, np.ndarray):
        return block.copy()

    # Create new block, with the values tensor converted to a torch tensor.
    new_block = TensorBlock(
        values=np.array(block.values.detach().numpy(), dtype=np.float64),
        samples=block.samples,
        components=block.components,
        properties=block.properties,
    )

    # Add gradients to each block, again where the values tensor are torch
    # tensors.
    for parameter, gradient in block.gradients():
        new_block.add_gradient(
            parameter,
            np.array(gradient.data.detach().numpy(), dtype=np.float64),
            gradient.samples,
            gradient.components,
        )

    return new_block


def make_contiguous_numpy(tensor: TensorMap) -> TensorMap:
    """
    Takes a TensorMap whose block values are ndarrays and ensures they are
    contiguous. Allows tensors produced by slicing/splitting to be saved to file
    using the equistore.io.save method.
    """

    new_blocks = []
    for key, block in tensor:
        new_block = TensorBlock(
            samples=block.samples,
            components=block.components,
            properties=block.properties,
            values=np.ascontiguousarray(block.values),
        )
        for parameter, gradient in block.gradients():
            new_block.add_gradient(
                parameter=parameter,
                samples=gradient.samples,
                components=gradient.components,
                data=np.ascontiguousarray(gradient.data),
            )
        new_blocks.append(new_block)

    return TensorMap(
        keys=tensor.keys,
        blocks=new_blocks,
    )


# ===== fxns converting types of Labels object entries:
# i.e. np.void <--> tuple <--> str


def key_to_str(key: Union[np.void, tuple]) -> str:
    """
    Takes a single numpy void key (i.e. an element of a Labels object) or a
    tuple and converts it to a string. Return string has the form
    f"{key[0]}_{key[1]}_...", where element values are separated by undescores.
    """
    return "_".join([str(k) for k in key])


def key_npvoid_to_tuple(key: np.void) -> tuple:
    """
    Takes a single numpy void key (i.e. an element of a Labels object) and
    converts it to a tuple.
    """
    return tuple(k for k in key)


def key_tuple_to_npvoid(key: tuple, names: List[str]) -> np.void:
    """
    Takes a key as a tuple and the associated names of the values in that tuple
    and returns a numpy void object that can be used to access blocks in a
    TensorMap, as well as values in a dict that are indexed by these numpy void
    keys.
    """
    # We need to create a TensorMap object here, as this allows a hashable object that
    # can be used to access dict values to be returned.
    tensor = TensorMap(
        keys=Labels(
            names=names,
            values=np.array(
                [key],
                dtype=np.int32,
            ),
        ),
        blocks=[
            TensorBlock(
                values=np.full((1, 1), 0.0),
                samples=Labels(
                    ["s"],
                    np.array(
                        [
                            [0],
                        ],
                        dtype=np.int32,
                    ),
                ),
                components=[],
                properties=Labels(["p"], np.array([[0]], dtype=np.int32)),
            )
        ],
    )
    return tensor.keys[0]


# ===== fxns for equistore Labels objects comparisons


def searchable_labels(labels: Labels):
    """
    Returns the input Labels object but after being used to construct a
    TensorBlock, so that look-ups can be performed.
    """
    return TensorBlock(
        values=np.full((len(labels), 1), 0.0),
        samples=labels,
        components=[],
        properties=Labels(["p"], np.array([[0]], dtype=np.int32)),
    ).samples


def labels_equal(a: Labels, b: Labels, correct_order: bool = True):
    """
    For 2 :py:class:`Labels` objects ``a`` and ``b``, returns true if they are
    exactly equivalent in names and values. If ``correct_order=True`` (the
    default), checks also that the order of the elements in the Labels is
    exactly equivalent.
    """
    # They can only be equivalent if the same length
    if len(a) != len(b):
        return False
    if not correct_order:
        a, b = np.sort(a), np.sort(b)
    return np.all(np.array(searchable_labels(a) == searchable_labels(b)))


def labels_intersection(a: Labels, b: Labels):
    """
    For 2 :py:class:`Labels` objects ``a`` and ``b``, returns a new Labels
    object of the indices that they share, i.e. the intersection.
    """
    # Create dummy TensorBlocks with a and b as the samples labels
    a = searchable_labels(a)
    b = searchable_labels(b)
    # Find the intersection
    intersection_idxs = [i for i in [a.position(b_i) for b_i in b] if i is not None]
    return a[intersection_idxs]


def get_feature_labels(tensor: Union[TensorMap, TensorBlock]) -> Union[dict, Labels]:
    """
    Returns the feature labels for the input ``tensor``. If passing a TensorMap,
    this function returns a dict of Labels object, indexed by the block keys,
    with values that correspond to the properties Labels of each block. If
    passing just a TensorBlock, a Labels object corresponding to the input
    ``tensor`` properties Labels is returned.
    """
    if isinstance(tensor, TensorMap):
        return {key: block.properties for key, block in tensor}
    elif isinstance(tensor, TensorBlock):
        return tensor.properties
    else:
        raise TypeError("``tensor`` must be either a TensorMap or TensorBlock")


# ===== TensorMap + TensorBlock functions


def num_elements_tensor(tensor: Union[TensorMap, TensorBlock, torch.Tensor]) -> int:
    """
    Returns the total number of elements in the input tensor.

    If the input tensor is a TensorMap the number of elements is given by the
    sum of the product of the dimensions for each block.

    If the input tensor is a TensorBlock or a torch.Tensor, the number of
    elements is just given by the product of the dimensions.
    """
    if isinstance(tensor, TensorMap):
        return torch.sum([torch.prod(block.values.shape) for _, block in tensor])
    elif isinstance(tensor, TensorBlock):
        return torch.prod(tensor.values.shape)
    elif isinstance(tensor, torch.Tensor):
        return torch.prod(tensor.size)
    else:
        raise TypeError("must pass either a TensorMap, TensorBlock, or torch.Tensor")


def delta_tensor(input: TensorMap, target: TensorMap, absolute: bool = False):
    """
    Returns a :py:class:`TensorMap` whose block values are the difference
    between the block values of the input and target tensors. The keys,
    samples, components and properties Labels must be the same for both
    tensors. For each :py:class:`TensorBlock` in the ``input`` tensor, the
    values of the corresponding :py:class:`TensorBlock` in the ``target``
    tensor is subtracted, i.e. input_block.values - target_block.values for
    each block in the input/target TensorMaps.

    If ``absolute`` is true, the absolute difference |input - target|.
    """
    # Check tensor metadata is equivalent
    equivalent_metadata(input, target)

    # Calculate the delta tensor
    blocks = []
    for key in target.keys:
        values = input[key].values - target[key].values
        if absolute:
            values = abs(values)
        blocks.append(
            TensorBlock(
                samples=target[key].samples,
                components=target[key].components,
                properties=target[key].properties,
                values=values,
            )
        )
    delta = TensorMap(keys=target.keys, blocks=blocks)
    return delta


def rename_tensor(
    tensor: TensorMap,
    keys_names: Optional[List[str]] = None,
    samples_names: Optional[List[str]] = None,
    components_names: Optional[List[str]] = None,
    properties_names: Optional[List[str]] = None,
) -> TensorMap:
    """
    Constructs and returns a new TensorMap where the names of the Labels
    metadata for the keys, samples, components, and/or properties have been
    modified according to the new names. Note: does not yet handle gradients.
    """
    new_keys = tensor.keys
    # Modify key names
    if keys_names is not None:
        if len(keys_names) != len(tensor.keys.names):
            raise ValueError(
                "must pass the same number of new keys names as there are old ones"
            )
        new_keys = Labels(
            names=keys_names,
            values=np.array([[i for i in k] for k in tensor.keys]),
        )
    if samples_names is None and components_names is None and properties_names is None:
        new_blocks = [tensor[key].copy() for key in tensor.keys]
    else:
        new_blocks = [
            rename_block(
                tensor[key],
                samples_names=samples_names,
                components_names=components_names,
                properties_names=properties_names,
            )
            for key in tensor.keys
        ]
    return TensorMap(keys=new_keys, blocks=new_blocks)


def rename_block(
    block: TensorBlock,
    samples_names: Optional[List[str]] = None,
    components_names: Optional[List[str]] = None,
    properties_names: Optional[List[str]] = None,
) -> TensorBlock:
    """
    Constructs and returns a new TensorBlock where the names of the Labels
    metadata for samples, components, and/or properties have been modified
    according to the new names. Note: does not yet handle gradients.
    """
    new_samples = block.samples
    new_components = block.components
    new_properties = block.properties
    # Modify samples names
    if samples_names is not None:
        if len(samples_names) != len(block.samples.names):
            raise ValueError(
                "must pass the same number of new samples names as there are old ones"
            )
        samp_values = np.array(
            [i for s in block.samples for i in s], dtype=np.int32
        ).reshape(-1, len(samples_names))
        new_samples = Labels(
            names=samples_names,
            values=samp_values,
        )
    # Modify components names
    if components_names is not None:
        if len(block.components) != len(components_names):
            raise ValueError("must pass same number of new components as old")
        new_components = []
        for c_i in range(len(block.components)):
            if len(components_names[c_i]) != len(block.components[c_i].names):
                raise ValueError(
                    "must pass the same number of new components names as there are old ones"
                )
            comp_values = np.array(
                [j for i in block.components[c_i] for j in i], dtype=np.int32
            ).reshape(-1, len(components_names[c_i]))
            new_components.append(
                Labels(
                    names=components_names[c_i],
                    values=comp_values,
                )
            )
    # Modify properties names
    if properties_names is not None:
        if len(properties_names) != len(block.properties.names):
            raise ValueError(
                "must pass the same number of new properties names as there are old ones"
            )
        prop_values = np.array(
            [i for p in block.properties for i in p], dtype=np.int32
        ).reshape(-1, len(properties_names))
        new_properties = Labels(
            names=properties_names,
            values=prop_values,
        )

    return TensorBlock(
        samples=new_samples,
        components=new_components,
        properties=new_properties,
        values=block.values,
    )


def drop_key_name(tensor: TensorMap, key_name: str) -> TensorMap:
    """
    Takes a TensorMap and drops the key_name from the keys. Every key must have
    the same value for the key_name, otherwise a ValueError is raised.
    """
    # Check that the key_name is present and unique
    if not len(np.unique(tensor.keys[key_name])) == 1:
        raise ValueError(
            f"key_name {key_name} is not unique in the keys."
            " Can only drop a key_name where the value is the"
            " same for all keys."
        )

    # Define the idx of the key_name to drop
    drop_idx = tensor.keys.names.index(key_name)

    # Build the new keys
    new_keys = Labels(
        names=tensor.keys.names[:drop_idx] + tensor.keys.names[drop_idx + 1 :],
        values=np.array(
            [k.tolist()[:drop_idx] + k.tolist()[drop_idx + 1 :] for k in tensor.keys]
        ),
    )
    return TensorMap(keys=new_keys, blocks=[b.copy() for b in tensor.blocks()])


def drop_metadata_name(tensor: TensorMap, axis: str, name: str) -> TensorMap:
    """
    Takes a TensorMap and drops the `name` from either the "samples" or
    "properties" labels of every block. Every block must have the same value for
    the `name`, otherwise a ValueError is raised.
    """
    if axis not in ["samples", "properties"]:
        raise ValueError(f"axis must be 'samples' or 'properties', not {axis}")
    # Check that the name is present and unique
    for block in tensor.blocks():
        if axis == "samples":
            uniq = np.unique(block.samples[name])
        else:
            uniq = np.unique(block.properties[name])
        if not len(uniq) == 1:
            raise ValueError(
                f"name {name} is not unique in the {axis}."
                " Can only drop a `name` where the value is the"
                f" same for all {axis}."
            )
    # Identify the idx of the name to drop
    if axis == "samples":
        drop_idx = tensor.blocks()[0].samples.names.index(name)
    elif axis == "properties":
        drop_idx = tensor.blocks()[0].properties.names.index(name)
    # Construct new blocks with the dropped name
    new_blocks = []
    for key in tensor.keys:
        new_samples = tensor[key].samples
        new_properties = tensor[key].properties
        if axis == "samples":
            new_samples = Labels(
                names=tensor[key].samples.names[:drop_idx]
                + tensor[key].samples.names[drop_idx + 1 :],
                values=np.array(
                    [
                        s.tolist()[:drop_idx] + s.tolist()[drop_idx + 1 :]
                        for s in tensor[key].samples
                    ]
                ),
            )
        else:
            new_properties = Labels(
                names=tensor[key].properties.names[:drop_idx]
                + tensor[key].properties.names[drop_idx + 1 :],
                values=np.array(
                    [
                        p.tolist()[:drop_idx] + p.tolist()[drop_idx + 1 :]
                        for p in tensor[key].properties
                    ]
                ),
            )
        new_blocks.append(
            TensorBlock(
                samples=new_samples,
                components=tensor[key].components,
                properties=new_properties,
                values=tensor[key].values,
            )
        )
    return TensorMap(
        keys=tensor.keys,
        blocks=new_blocks,
    )


def drop_blocks(tensor: TensorMap, keys: Labels) -> TensorMap:
    """
    Drop specified key/block pairs from a TensorMap.

    :param tensor: the TensorMap to drop the key-block pair from.
    :param keys: the key Labels of the blocks to drop

    :return: the input TensorMap with the specified key/block pairs dropped.
    """
    if not np.all(tensor.keys.names == keys.names):
        raise ValueError(
            "The input tensor's keys must have the same names as the specified"
            f" keys to drop. Should be {tensor.keys.names} but got {keys.names}"
        )
    new_keys = np.setdiff1d(tensor.keys, keys)
    return TensorMap(keys=new_keys, blocks=[tensor[key].copy() for key in new_keys])


def pad_with_empty_blocks(
    input: TensorMap, target: TensorMap, slice_axis: str = "samples"
) -> TensorMap:
    """
    Takes an ``input`` TensorMap with fewer blocks than the ``target``
    TensorMap. For every key present in ``target`` but not ``input``, an empty
    block is created by slicing the ``target`` block to zero dimension along the
    ``slice_axis``, which is either "samples" or "properties". For every key
    present in both ``target`` and ``input``, the block is copied exactly from
    ``input``. A new TensorMap, with the same number of blocks at ``target``,
    but all the original data from ``input``, is returned.
    """
    blocks = []
    for key in target.keys:
        if key in input.keys:
            # Keep the block
            blocks.append(input[key].copy())
        else:
            samples = target[key].samples
            properties = target[key].properties
            # Create an empty sliced block
            if slice_axis == "samples":
                values = target[key].values[:0]
                samples = samples[:0]
            else:  # properties
                target[key].values[..., :0]
                properties = properties[:0]
            blocks.append(
                TensorBlock(
                    samples=samples,
                    components=target[key].components,
                    properties=properties,
                    values=values,
                )
            )
    return TensorMap(keys=target.keys, blocks=blocks)


def equal_metadata(
    tensor_1: TensorMap, tensor_2: TensorMap, check: Optional[List] = None
) -> bool:
    """
    Checks if two :py:class:`TensorMap` objects have the same metadata,
    returning a bool.

    The equivalence of the keys of the two :py:class:`TensorMap` objects is always
    checked. If `check` is none (the default), all metadata (i.e. the samples,
    components, and properties of each block) is checked to contain the
    same values in the same order.

    Passing `check` as a list of strings will only check the metadata specified.
    Allowed values to pass are "samples", "components", and "properties".

    :param tensor_1: The first :py:class:`TensorMap`.
    :param tensor_2: The second :py:class:`TensorMap` to compare to the first.
    :param check: A list of strings specifying which metadata of each block to
        check. If none, all metadata is checked. Allowed values are "samples",
        "components", and "properties".

    :return: True if the metadata of the two :py:class:`TensorMap` objects are
        equal, False otherwise.
    """
    # Check input args
    if not isinstance(tensor_1, TensorMap):
        raise TypeError(f"`tensor_1` must be a TensorMap, not {type(tensor_1)}")
    if not isinstance(tensor_2, TensorMap):
        raise TypeError(f"`tensor_2` must be a TensorMap, not {type(tensor_2)}")
    if not isinstance(check, (list, type(None))):
        raise TypeError(f"`check` must be a list, not {type(check)}")
    if check is None:
        check = ["samples", "components", "properties"]
    for metadata in check:
        if not isinstance(metadata, str):
            raise TypeError(
                f"`check` must be a list of strings, got list of {type(metadata)}"
            )
        if metadata not in ["keys", "samples", "components", "properties"]:
            raise ValueError(f"Invalid metadata to check: {metadata}")
    # Check equivalence in keys
    try:
        _utils._check_maps(tensor_1, tensor_2, "equal_metadata")
    except ValueError:
        return False

    # Loop over the blocks
    for key in tensor_1.keys:
        block_1 = tensor_1[key]
        block_2 = tensor_2[key]

        # Check metatdata of the blocks
        try:
            _utils._check_blocks(block_1, block_2, check, "equal_metadata")
        except ValueError:
            return False

        # Check metadata of the gradients
        try:
            _utils._check_same_gradients(block_1, block_2, check, "equal_metadata")
        except ValueError:
            return False
    return True


# ===== other utility functions


def get_log_subset_sizes(
    n_max: int,
    n_subsets: int,
    base: Optional[float] = np.e,
) -> np.array:
    """
    Returns an ``n_subsets`` length array of subset sizes equally spaced along a
    log of specified ``base`` (default base e) scale from 0 up to ``n_max``.
    Elements of the returned array are rounded to integer values. The final
    element of the returned array may be less than ``n_max``.
    """
    # Generate subset sizes evenly spaced on a log (base e) scale
    subset_sizes = np.logspace(
        np.log(n_max / n_subsets),
        np.log(n_max),
        num=n_subsets,
        base=base,
        endpoint=True,
        dtype=int,
    )
    return subset_sizes


# ===== feature standardization


def get_invariant_means(tensor: TensorMap) -> TensorMap:
    """
    Calculates the mean of the invariant (l=0) blocks on the input `tensor`
    using the `equistore.mean_over_samples` function. Returns the result in a
    new TensorMap, whose number of block is equal to the number of invariant
    blocks in `tensor`. Assumes `tensor` is a numpy-based TensorMap.
    """
    # Define the keys of the invariant blocks and create a new TensorMap
    inv_keys = tensor.keys[tensor.keys["spherical_harmonics_l"] == 0]
    inv_tensor = TensorMap(keys=inv_keys, blocks=[tensor[k].copy() for k in inv_keys])

    # Find the mean over sample for the invariant blocks
    return equistore.mean_over_samples(
        inv_tensor, samples_names=inv_tensor.sample_names
    )


def standardize_invariants(
    tensor: TensorMap, invariant_means: TensorMap, reverse: bool = False
) -> TensorMap:
    """
    Standardizes the invariant (l=0) blocks on the input `tensor` by subtracting
    from each coefficient the mean of the coefficients belonging to that
    feature.

    Must pass the TensorMap containing the means of the features,
    `invariant_means`. If `reverse` is true, the mean is instead added back to
    the coefficients of each feature. Assumes `tensor` and `invariant_means` are
    numpy-based TensorMaps.
    """
    # Iterate over the invariant keys
    for inv_key in invariant_means.keys:
        block = tensor[inv_key]
        # Iterate over each feature/property
        for p in range(len(block.properties)):
            if reverse:  # add the mean to the values
                block.values[..., p] += invariant_means[inv_key].values[..., p]
            else:  # subtract
                block.values[..., p] -= invariant_means[inv_key].values[..., p]

    return tensor
